# IR_Final_Project

This project is a web scraping and content extraction pipeline for collecting and processing opportunity listings from [opportunitiescircle.com](https://www.opportunitiescircle.com/). It also includes a recommendation system based on Latent Semantic Indexing (LSI) and user ratings, accessible via both a command-line interface (CLI) and a graphical user interface (GUI).

## Directory Structure

```
IR_Final_Project/
├── requirements.txt           # Python dependencies
├── README.md                  # This readme file
├── init.sh                    # Initialization script (if any)
├── evaluation.sh              # Evaluation Model
├── launch_app.sh              # Run the app
├── grep/
│   ├── page.py                # Scrapes paginated listing pages and saves HTML
│   ├── activity.py            # Extracts opportunity links from listing HTML files
│   ├── activity_html.py       # Downloads each opportunity's detail page HTML
│   ├── content.py             # Extracts and saves main text content from each detail HTML
│   ├── build_lsi.py           # Builds term-document matrix and generates LSI document/term vectors
│   ├── baseline_recommender.py # Generates recommendations for departments based on LSI vectors
│   ├── find_similar_docs.py   # Finds top 5 most relevant documents using LSI embeddings
│   ├── fetch_missing_activites.py # Fetch all missing item from train data
│   ├── user_rating.py         # CLI script for users to rate opportunities and generate user vectors (saves to grep/users.json)
│   ├── department.txt         # Input file listing departments (manual or external)
│   ├── term_data_lsi.npz      # LSI term data (terms and vectors) (generated by build_lsi.py)
│   ├── doc_data_lsi.npz       # LSI document data (file names and vectors) (generated by build_lsi.py)
│   ├── departments_lsi.npy    # LSI vectors for departments (generated by baseline_recommender.py)
│   ├── users.json             # Stores CLI user names, ratings, and vectors (generated by user_rating.py)
│   ├── activity_data/         # Downloaded HTML files for each opportunity
│   ├── activity_data_text/    # Extracted text content for each opportunity
│   └── page_data/             # Downloaded HTML files for each listing page
├── gui/
│   ├── app.py                 # Flask application for the GUI
│   ├── users.json             # Stores user profiles and ratings for the GUI
│   └── templates/             # HTML templates for the GUI
│       ├── index.html         # User name input page
│       ├── profile.html       # User profile creation page
│       ├── recommendations.html # Activity rating page
│       └── thank_you.html     # Thank you page
|── method2/                   # Runing the training of BPR
│   ├── cold_start_model.py    
│   ├── baseline.py   
|── evaluation/                # Evaluation of our model
│   ├── abtest.py
│   ├── evaluation.py
│   ├── gemini_evaluation_results
│   ├── pseudo_rating.py
│   ├── tau_b.py
```

## Usage
1. You need a python environment
2. Run following command to grep data and train model.
```bash
./init.sh
```
3. You can use following command to evaluation the model.
```bash
./evaluation.sh
```

## Running the Web-based GUI Recommender

This project also includes a Flask-based web interface for user registration, profile creation, and activity rating.

**Prerequisites:**

*   Ensure you have installed all dependencies as described in **Usage Step 1 (Install dependencies)**. This includes Flask, which should be covered by `pip install -r requirements.txt`.
*   The GUI relies on data generated by the `grep/` pipeline. You **must** run **Usage Steps 2 through 6** (`grep/page.py`, `grep/activity.py`, `grep/activity_html.py`, `grep/content.py`, `grep/build_lsi.py`) to download activity data, extract content, and generate the necessary LSI models (specifically `grep/doc_data_lsi.npz` and the contents of `grep/activity_data_text/`).

**Running the GUI:**

1.  Make sure you are in the project root directory (`IR_Final_Project/`).
2.  Run the Flask application using the following command:
    ```bash
    ./launch_app.sh
    ```
3.  Open your web browser and navigate to `http://127.0.0.1:5001` (or `http://localhost:5001`).

The GUI allows new users to:
*   Register with their name.
*   Create a profile by specifying their age group, department, and country of interest.
*   Receive and rate a selection of 10 randomly presented activities.
User profiles and their ratings are stored in `gui/users.json`.

## method2
In this part, we want to use bpr, make sure you already do fetch_missing_activities.py and content.py. And make sure no any .plk file in method2/.
- If you want to train our model
```bash
python method2/cold_start_model.py
```
- If you want to train baseline model
```bash
python method2/baseline.py
```

## Notes
-   All scripts are written in Python and primarily use `requests` and `BeautifulSoup` for web scraping and parsing, and `numpy` for numerical operations.
-   The GUI is built using Flask.
-   The pipeline is modular; you can run each step independently, assuming the required input files from previous steps exist.
-   The output folders (`grep/page_data/`, `grep/activity_data/`, `grep/activity_data_text/`) will be created automatically if they do not exist when running the `grep/` scripts.