# IR_Final_Project

This project is a web scraping and content extraction pipeline for collecting and processing opportunity listings from [opportunitiescircle.com](https://www.opportunitiescircle.com/). It also includes a recommendation system based on Latent Semantic Indexing (LSI) and user ratings.

## Directory Structure

```
IR_Final_Project/
├── requirements.txt           # Python dependencies
├── README.md                  # This readme file
├── init.sh                    # Initialization script (if any)
├── grep/
│   ├── page.py                # Scrapes paginated listing pages and saves HTML
│   ├── activity.py            # Extracts opportunity links from listing HTML files
│   ├── activity_html.py       # Downloads each opportunity's detail page HTML
│   ├── content.py             # Extracts and saves main text content from each detail HTML
│   ├── build_lsi.py           # Builds term-document matrix and generates LSI document/term vectors
│   ├── baseline_recommender.py # Generates recommendations for departments based on LSI vectors
│   ├── find_similar_docs.py   # Finds top 5 most relevant documents using LSI embeddings
│   ├── user_rating.py         # Script for users to rate opportunities and generate user vectors
│   ├── department.txt         # Input file listing departments (manual or external)
│   ├── term_data_lsi.npz      # LSI term data (terms and vectors) (generated by build_lsi.py)
│   ├── doc_data_lsi.npz       # LSI document data (file names and vectors) (generated by build_lsi.py)
│   ├── departments_lsi.npy    # LSI vectors for departments (generated by baseline_recommender.py)
│   ├── users.json             # Stores user names, ratings, and vectors (generated by user_rating.py)
│   ├── activity_data/         # Downloaded HTML files for each opportunity
│   ├── activity_data_text/    # Extracted text content for each opportunity
│   └── page_data/             # Downloaded HTML files for each listing page
└── CLCRec/
    ├── ...                    # Sub-project for Contrastive Learning for Cold-start Recommendation
```

## Usage

1.  **Install dependencies**
    ```bash
    pip install torch==2.7.0 # Or a compatible version
    pip install torch_scatter==2.1.2 -f https://data.pyg.org/whl/torch-2.7.0+cu128.html # Adjust for your PyTorch and CUDA version
    pip install -r requirements.txt
    ```

2.  **Step 1: Download listing pages**
    -   Run `grep/page.py` to download paginated opportunity listings (pages 1–25) into `grep/page_data/`.
    ```bash
    python grep/page.py
    ```

3.  **Step 2: Extract opportunity links**
    -   Run `grep/activity.py` to parse each listing page and collect unique opportunity links into `grep/activity_data/opportunity_links.txt`.
    ```bash
    python grep/activity.py
    ```

4.  **Step 3: Download opportunity detail pages**
    -   Run `grep/activity_html.py` to download each opportunity's detail HTML into `grep/activity_data/`.
    ```bash
    python grep/activity_html.py
    ```

5.  **Step 4: Extract main content**
    -   Run `grep/content.py` to extract and save the main text content from each HTML file into `grep/activity_data_text/`.
    ```bash
    python grep/content.py
    ```

6.  **Step 5: Generate LSI vectors**
    -   Run `grep/build_lsi.py` to build the term-document matrix, apply Latent Semantic Indexing (LSI), and generate term data (`grep/term_data_lsi.npz`) and document data (`grep/doc_data_lsi.npz`).
    ```bash
    python grep/build_lsi.py
    ```

7.  **Step 6: Generate Department Recommendations (Baseline)**
    -   Run `grep/baseline_recommender.py` to generate LSI vectors for departments based on `grep/department.txt`, `grep/term_data_lsi.npz`, and `grep/doc_data_lsi.npz`.
    -   The department LSI vectors are saved to `grep/departments_lsi.npy`.
    ```bash
    python grep/baseline_recommender.py
    ```

8.  **Step 7: Find similar documents**
    -   To find the top 5 most relevant documents to a given file, use the interactive script:
    ```bash
    python grep/find_similar_docs.py
    ```
    Enter a filename from `grep/activity_data_text` (e.g. `commonwealth-distance-learning-scholarships.txt`) when prompted, and the script will output the five most similar documents based on LSI embeddings.

9.  **Step 8: User-based Ratings and Recommendations (Experimental)**
    -   Run `grep/user_rating.py` to allow a user to rate a sample of 10 activities.
    -   This script will then initialize a user vector based on these ratings and save user data (name, ratings, vector) to `grep/users.json`.
    -   Note: The script currently indicates it does not serve returning users.
    ```bash
    python grep/user_rating.py
    ```

## Notes
-   All scripts are written in Python and primarily use `requests` and `BeautifulSoup` for web scraping and parsing, and `numpy` for numerical operations.
-   The pipeline is modular; you can run each step independently, assuming the required input files from previous steps exist.
-   The output folders (`grep/page_data/`, `grep/activity_data/`, `grep/activity_data_text/`) will be created automatically if they do not exist.

## License
This project is for educational and research purposes only.

## CLCRec Sub-project
The `CLCRec/` directory contains a separate project implementing "Contrastive Learning for Cold-start Recommendation". Please refer to `CLCRec/README.md` for details on that specific project.

@inproceedings{CLCRec,
  title     = {Contrastive Learning for Cold-start Recommendation},
  author    = {Wei, Yinwei and
               Wang, Xiang and
               Qi, Li and
               Nie, Liqiang and
               Li, Yan and
               Li, Xuanqing and
               Chua, Tat-Seng},
  booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
  pages     = {--},
  year      = {2021}
}